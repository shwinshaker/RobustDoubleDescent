# ---- preset paras ---- (for config only)
test: False # True
suffix: epoch=1000

# ----- adversary ---------
adversary: pgd # gairat # trades # pgd # trades # trades # pgd # mart # trades # pgd # fgsm # pgd # gaussian #  pgd #  fgsm 
eps: 8 # 16 # 30 
pgd_alpha: 2 # 0.5
pgd_iter: 10 # 20
alpha: 1.0 # 
rand_init: True
fat_taus: [10, 0, 0] # 
fat_milestones: [31, 61] # GAIRAT setting
target:

ad_test: pgd # fgsm # pgd # Support multiple evaluation like 'fgsm, gaussian': [s.strip() for s in ('fgsm, gaussian').split(',')]
eps_test: 8 # 24 for mnist
pgd_alpha_test: 2 # 1 # 2
pgd_iter_test: 10 # 5


# ---- knowledge distillation
kd: True
kd_teacher_st: 
# kd_teacher_rb: 'sgd_PreActResNet18_gain=1_0_ad_trades_10_wd=0_0005_mom=0_9_pgd_10/best_model.pt'
kd_teacher_rb: 'sgd_PreActResNet18_gain=1_0_ad_pgd_10_alpha=1_wd=0_0005_mom=0_9_pgd_10-0/best_model.pt'
# kd_teacher_rb: 'cifar100_sgd_PreActResNet18_gain=1_0_ad_pgd_10_alpha=1_wd=0_0005_mom=0_9_pgd_10/best_model.pt'
# kd_teacher_rb: 'tiny-imagenet_sgd_PreActResNet18_gain=1_0_ad_pgd_10_alpha=1_wd=0_0005_mom=0_9_pgd_10/best_model.pt'
kd_coeff_st: 0. # 0.1 # 0.25
kd_coeff_rb: 0.8 # 0.85 # 0.83
kd_temperature: 1.47 # 1.23 # 1.53 # 1.72

# -- tracker --- 
paraTrack: False # True
lrTrack: False # True
lipTrack: False #  True
resTrack: False # True # False # True
rbTrack: [] # ['FGSM'] # , 'CW']
rbTrackPhase: 'train'
rbTrackSubsize: 1000
mrTrack: False # True
adTrack: False # True
# would be extremely slow for TRADES because have to generate ad examples twice
# not implemented yet for subset training -- need to track indices in the subset
confTrack: False
confTrackOptions: []

# --initializor --
warmup: 0 # 5 # epochs

# -- regularizor --
## manifold regularization
lmr: 0. # 10. #  0.125 # 10. #  10. # :100000.0 #  10000.0 # 10.0 1.0

swa: True
swa_start: 80
swa_interval: 1

class_eval: False # True

# -- data --
# train_subset_path: '/home/chengyu/RobustDoubleDescent/data_subsets/mean_rb_id_pgd10_epoch_friend_10000_tiny-imagenet.npy'
# train_subset_path: '/home/chengyu/RobustDoubleDescent/data_subsets/mean_rb_id_pgd10_epoch_friend_10000.npy'

# eval_subset_path: ['/home/chengyu/Initialization/data_subsets/mean_rb_id_pgd10_early_stop_problem.npy',
#                '/home/chengyu/Initialization/data_subsets/mean_rb_id_pgd10_early_stop_friend.npy',]


# ---- weights must be consistent with alpha! ----------
# ---- alpha will not take any effect if set this! ------
# alpha_sample_path: '/home/chengyu/Initialization/data_subsets/mean_rb_binary.npy'
# lambda_sample_path: '/home/chengyu/Initialization/data_subsets/mean_rb_lambda_pgd10_epoch_tanh+1.npy'
# weps_sample_path: '/home/chengyu/Initialization/data_subsets/mean_rb_weps_pgd10_epoch_binary_40k_+1_25-0_5.npy'
# num_iter_sample_path: '/home/chengyu/Initialization/data_subsets/mean_rb_num_iter_pgd10_epoch_binary_2_4_8.npy'
## Setting this will make both pgd_iter and pgd_alpha invalid

# ---- weight start at some epoch
# ---- will not work if weighted training not enabled
# alpha_sample_path2: '/home/chengyu/Initialization/data_subsets/hard_5000_weights.npy'
# alpha_sample_path2: '/home/chengyu/Initialization/data_subsets/hard_10000_weights.npy'
# alpha_sample_path2: '/home/chengyu/Initialization/data_subsets/easy_10000_weights.npy'


# -- regularizer --
# label_smoothing: 0.5
# reg_sample_path: '/home/chengyu/Initialization/data_subsets/mean_smoothing_binary_10000_r.npy'
# loss_flooding: 1.5
# reg_sample_path: '/home/chengyu/Initialization/data_subsets/hard_5000_flooding.npy'

# ----- Regular paras ------
dataset: cifar10 # mnist # cifar100 # tiny-imagenet 
# aux_data: 'ti_500K_pseudo_labeled.pickle'
soft_label: False
classes: #  ['dog', 'cat']
trainsize:
testsize:
valsize:
data_dir: '/home/chengyu/RobustDoubleDescent/data'
opt: sgd # adam # sgd # adam # rmsprop # adagrad 
model: PreActResNet18 # logistic # resnet #_woskip # dln11 # vgg11 # dln11 # resnet_fixup # dln11 # resnet_woskip # vgg11 # logistic #  resnet # _woaffine # _wobn # vgg11 # _bn # resnet  #
bn: True # False
depth: 20
width: 64
scheduler: multistep
resume: False
epochs: 1000 # 110 # 80 # 100 # 200 
milestones: [80, 120] # [100, 105] # [30, 60]  # [100, 150] #  step lr scheduler
lr: 0.1 # 0.001 # - adam
wd: 0.0005 # 05 # # 0
momentum: 0.9 # .9 # .9   # momentum seems to cause instability 
batch_size: 128 # 32
gamma: 0.1 # lr decay factor

gpu_id: '0,1'
manual_seed: # 7
model_seed:
state_path: #  'sgd_resnet20_gain=1_0_ad_pgd_5_wd=0_0005_mom=0_9/model.pt'
best: 'robust'
traintest: False # True
save_model: True
save_model_at: [160, 200, 300, 400, 500, 600, 700, 800, 900]
save_checkpoint: False


